{"file_contents":{"backend/utils/logger.py":{"content":"","size_bytes":0},"style.css":{"content":"*{\n    margin: 0;\n    padding: 0;\n    box-sizing: border-box;\n}\n\n:root {\n  --dark-brown: #3b2f2f;\n  --warm-tan: #a78b6d;\n  --white: #ffffff;\n  --text-dark: #2c2c2c;\n  --text-light: #f5f5f5;\n  --shadow: rgba(0, 0, 0, 0.2);\n}\n\n\nbody {\n  font-family: \"Inter\", -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, sans-serif;\n  background: linear-gradient(180deg, var(--dark-brown) 0%, var(--warm-tan) 60%, var(--white) 100%);\n  color: var(--text-dark);\n  line-height: 1.6;\n  overflow-x: hidden;\n  min-height: 100vh;\n}\n\n\n.hero{\n    width: auto;\n    height: 100vh;\n    background-color: aqua;\n}\n\n.container{\n    background-color: blue;\n}","size_bytes":634},"backend/database/__init__.py":{"content":"","size_bytes":0},"backend/utils/__init__.py":{"content":"","size_bytes":0},"backend/routes/register.py":{"content":"","size_bytes":0},"backend/services/post_generator.py":{"content":"from db import get_connection\nfrom datetime import datetime\nfrom backend.database.models import get_all_posts\nfrom backend.database.models import get_scheduled_posts\nfrom backend.database.models import get_all_schedule_for_niche\n\ndef get_all_schedule():\n    conn = get_connection()\n    cursor = conn.cursor(dictionary=True)\n    cursor.execute(\"SELECT * FROM schedule\")\n    schedule = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return schedule\n\n\nschedule = get_all_schedule()\nfor s in schedule:\n    posts_per_day = s[\"posts_per_day\"]\n    print(posts_per_day)\nposts = get_all_posts()\nfor post in posts:\n    niche_id = post[\"niche_id\"]\n    scheduled_time = post[\"scheduled_time\"]\n    print(niche_id, scheduled_time)\n\n","size_bytes":729},"backend/services/ai_generator.py":{"content":"import requests\nfrom config import OPENAI_URL, OPENAI_API_KEY\n\ndef generate_post_content(niche):\n    API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n    API_KEY = \"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\"\n\n    payload = {\n        \"model\": \"openai/gpt-4o-mini\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": f\"#insert your prompt here.\"}\n        ]\n    }\n\n    headers = {\n        \"Authorization\": f\"Bearer {API_KEY}\",\n        \"Content-Type\": \"application/json\",\n        \"HTTP-Referer\": \"http://localhost\",\n        \"X-Title\": \"AutoPosterApp\"\n    }\n\n    try:\n        r = requests.post(API_URL, headers=headers, json=payload, timeout=20)\n        r.raise_for_status()\n        data = r.json()\n        return data[\"choices\"][0][\"message\"][\"content\"].strip()\n\n    except Exception as e:\n        print(\"Error generating content:\", e)\n        return \"default generated post for now\"\n","size_bytes":932},"backend/scheduler/job_scheduler.py":{"content":"import tweepy\nimport time\nimport schedule\nimport logging\nfrom datetime import datetime\nfrom config import X_API_KEY, X_API_SECRET, X_ACCESS_TOKEN, X_ACCESS_SECRET\nfrom backend.database.models import (\n    get_all_schedule_for_niche,\n    get_draft_posts_for_niche,\n    update_post_schedule,\n    get_due_posts,\n    update_post_status,\n)\nfrom backend.services.ai_generator import generate_post_content\n\n# ------------------- Logging Setup -------------------\nlogging.basicConfig(\n    filename=\"scheduler.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s — %(levelname)s — %(message)s\",\n)\n\n\n# ------------------- Time Slot Generator -------------------\ndef generate_time_slots(posts_per_day):\n    today = datetime.today().date()\n    start = datetime.combine(today, datetime.strptime(\"07:00\", \"%H:%M\").time())\n    end = datetime.combine(today, datetime.strptime(\"23:00\", \"%H:%M\").time())\n\n    if posts_per_day <= 0:\n        return []\n\n    duration = end - start\n    interval = duration / posts_per_day\n    slots = [(start + (interval * i)).strftime(\"%Y-%m-%d %H:%M:%S\") for i in range(posts_per_day)]\n    return slots\n\n\n# ------------------- Assign Posts -------------------\ndef assign_posts_to_slot():\n    schedule_data = get_all_schedule_for_niche(1)\n    logging.info(f\"schedule for niche: {schedule_data}\")\n\n    if not schedule_data:\n        logging.warning(\"No schedule found.\")\n        return\n\n    schedule_row = schedule_data[0]\n    posts_per_day = schedule_row[\"posts_per_day\"]\n    needs_approval = schedule_row[\"needs_approval\"]\n    slots = generate_time_slots(posts_per_day)\n    drafts = get_draft_posts_for_niche(1, posts_per_day)\n\n    if not drafts:\n        logging.warning(\"No draft posts found for scheduling.\")\n        return\n\n    for slot, draft in zip(slots, drafts):\n        niche_id = draft[\"niche_id\"]\n        ai_text = generate_post_content(niche_id)\n\n        try:\n            if needs_approval == 1:\n                update_post_schedule(draft[\"id\"], slot, status=\"draft\", content=ai_text)\n                logging.info(f\"Draft post {draft['id']} scheduled with AI content.\")\n            else:\n                update_post_schedule(draft[\"id\"], slot, status=\"scheduled\", content=ai_text)\n                logging.info(f\"Scheduled post {draft['id']} updated with AI content.\")\n        except Exception as e:\n            logging.error(f\"Error updating post {draft['id']}: {e}\")\n\n\n# ------------------- Process Due Posts -------------------\ndef process_due_posts():\n    due_posts = get_due_posts()\n    if not due_posts:\n        logging.info(\"No due posts found.\")\n        return\n\n    for due in due_posts:\n        needs_approval = due[\"needs_approval\"]\n        post_id = due[\"id\"]\n\n        try:\n            if needs_approval == 1 and due[\"status\"] == \"scheduled\":\n                update_post_status(post_id, \"pending approval\")\n                logging.info(f\"Post {post_id} set to pending approval.\")\n            elif due[\"status\"] == \"approved\":\n                publish_post(due)\n            else:\n                publish_post(due)\n                update_post_status(post_id, \"posted\")\n        except Exception as e:\n            logging.error(f\"Error processing post {post_id}: {e}\")\n\n\n# ------------------- Publish Post -------------------\ndef publish_post(post):\n    post_id = post[\"id\"]\n    content = post[\"content\"]\n\n    client = tweepy.Client(\n        consumer_key=X_API_KEY,\n        consumer_secret=X_API_SECRET,\n        access_token=X_ACCESS_TOKEN,\n        access_token_secret=X_ACCESS_SECRET,\n    )\n\n    try:\n        me = client.get_me()\n        logging.info(f\"Authenticated as: {me.data.username}\")\n\n        response = client.create_tweet(text=content)\n        logging.info(f\"Tweet successfully posted for post {post_id}: {response}\")\n        update_post_status(post_id, \"posted\")\n\n    except tweepy.TooManyRequests:\n        logging.warning(\"Rate limit reached. Retrying in 15 minutes...\")\n        time.sleep(900)\n        publish_post(post)\n    except Exception as e:\n        logging.error(f\"Error posting tweet {post_id}: {e}\")\n\n\n# ------------------- Run Scheduler -------------------\ndef run_scheduler():\n    schedule.every().day.at(\"00:00\").do(assign_posts_to_slot)\n    schedule.every(1).minutes.do(process_due_posts)\n\n    logging.info(\"Scheduler started... press CTRL+C to stop.\")\n    while True:\n        schedule.run_pending()\n        time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    run_scheduler()\n","size_bytes":4431},"test_db.py":{"content":"from db import get_connection\n\ntry:\n    conn = get_connection()\n    cursor = conn.cursor()\n    cursor.execute(\"SHOW TABLES;\")\n    print(\"Tables in database:\")\n    for table in cursor.fetchall():\n        print(table)\n    conn.close()\nexcept Exception as e:\n    print(\"Connection failed:\", e)\n","size_bytes":291},"backend/scheduler/__init__.py":{"content":"","size_bytes":0},"backend/services/x_api_service.py":{"content":"","size_bytes":0},"backend/services/__init__.py":{"content":"","size_bytes":0},"backend/database/models.py":{"content":"from db import get_connection\nimport psycopg2.extras\n\n#function to create niches\ndef create_niche(name, description, primary_statement):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"\"\" INSERT INTO niches (name, description, primary_statement)\n            VALUES (%s, %s, %s)\n            \"\"\" \n    values = (name, description, primary_statement)\n    cursor.execute(sql, values)\n    conn.commit()\n    cursor.close()\n    conn.close()\n    print(\"niche created successfully\")\n\n#function to fetch niches\ndef get_all_niches():\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    cursor.execute(\" SELECT * FROM niches \")\n    niches = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return niches\n\n#function to create hashtags\ndef create_hashtags(niche_id, tag):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"\"\" INSERT INTO  hashtags (niche_id, tag)\n            VALUES (%s, %s) \n            \"\"\"\n    values = (niche_id, tag)\n    cursor.execute(sql, values)\n    conn.commit()\n    cursor.close()\n    conn.close()\n    print(\"hashtags added successfully\")\n\n#function to fetch hashtags\ndef get_all_hashtags(niche_id):\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    value = (niche_id,)\n    cursor.execute(\"SELECT * FROM hashtags WHERE niche_id = %s\", (value))\n    niches = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return niches\n\n#function to create accounts\ndef create_account(platform, api_key, api_secret, access_token, access_secret):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"\"\" INSERT INTO platform_accounts (platform, api_key, api_secret, access_token, access_secret)\n            VALUES (%s, %s, %s, %s, %s) \n            \"\"\"\n    values = (platform, api_key, api_secret, access_token, access_secret)\n    cursor.execute(sql, values)\n    conn.commit()\n    cursor.close()\n    conn.close()\n    print(\"account created successfully\")\n\n#function to fetch all accounts\ndef get_all_accounts():\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    cursor.execute(\"SELECT * FROM platform_accounts\")\n    accounts = cursor.fetchone()\n    cursor.close()\n    conn.close()\n    return accounts\n\n#function to create posts\ndef create_post(niche_id, account_id, content, media_url=None, scheduled_time=None, status=\"draft\"):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"\"\" INSERT INTO posts (niche_id, account_id, content, media_url, scheduled_time, status)\n            VALUES (%s, %s, %s, %s, %s, %s)\n       \"\"\"\n    values = (niche_id, account_id, content, media_url, scheduled_time, status)\n    cursor.execute(sql, values)\n    conn.commit()\n    cursor.close()\n    conn.close()\n    print(\"post created successfully\")\n\n#function to fetch posts\ndef get_all_posts():\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    cursor.execute(\"SELECT * FROM posts\")\n    posts = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return posts\n\n#function to fetch scheduled posts\ndef get_scheduled_posts():\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    sql = \"\"\" SELECT * FROM posts\n            WHERE scheduled_time IS NOT NULL\n            AND scheduled_time > CURRENT_TIMESTAMP;\n            \"\"\"\n    cursor.execute(sql)\n    posts = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return posts\n\n#function to create schedule\ndef create_schedule(niche_id, posts_per_day, needs_approval):\n    conn = get_connection() \n    cursor = conn.cursor()\n    sql = \"\"\" INSERT INTO schedule (niche_id, posts_per_day, needs_approval) \n        VALUES (%s, %s, %s) \"\"\"\n    value = (niche_id, posts_per_day, needs_approval)\n    cursor.execute(sql, value)\n    conn.commit()\n    cursor.close()\n    conn.close()\n    print(\"scheduled posts inserted\") \n\n\n#function to fetch schedule for niche\ndef get_all_schedule_for_niche(niche_id):\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    cursor.execute(\"SELECT * FROM schedule WHERE niche_id = %s\", (niche_id,))\n    niche = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return niche\n\n#fetch schedule from the schedule table\n\ndef get_all_schedule():\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    cursor.execute(\"SELECT * FROM schedule\")\n    schedule = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return schedule\n\n#count scheduled posts for a niche \n\ndef count_scheduled_posts_for_niche_today(niche_id):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"SELECT COUNT(*) FROM posts WHERE niche_id = %s AND DATE(scheduled_time) = CURRENT_DATE;\"\n    cursor.execute(sql, (niche_id,))\n    count = cursor.fetchone()\n    result = count[0]\n    cursor.close()\n    conn.close()\n    return result    \n\n#fetch draft posts\ndef get_draft_posts_for_niche(niche_id, limit):\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    sql = \"SELECT * FROM posts WHERE niche_id = %s AND status = 'draft' ORDER BY created_at ASC LIMIT %s;\"\n    cursor.execute(sql, (niche_id, limit))\n    status = cursor.fetchall()\n    cursor.close() \n    conn.close()\n    return status\n\n#update scheduled time\ndef update_post_schedule(post_id, scheduled_time, status, content):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"\"\" UPDATE posts\n            SET scheduled_time = %s, status = %s, content = %s\n            WHERE id = %s;\n    \"\"\"\n    cursor.execute(sql, (scheduled_time, status, content, post_id))\n    conn.commit()\n    cursor.close()\n    conn.close()\n    print(\"schedule updtated successfully\")\n\n\n#fetch specific posts where status = schedule\ndef get_due_posts():\n    conn = get_connection()\n    cursor = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)\n    sql = \"\"\" SELECT p.*, s.needs_approval\n            FROM posts p\n            JOIN schedule s ON p.niche_id = s.niche_id\n            WHERE p.status = 'scheduled'\n            AND p.scheduled_time <= CURRENT_TIMESTAMP;\n        \"\"\"\n    cursor.execute(sql)\n    status = cursor.fetchall()\n    cursor.close()\n    conn.close()\n    return status\n\n\n#update post status\ndef update_post_status(post_id, status):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"\"\"\n            UPDATE posts\n            SET status = %s\n            WHERE id = %s;\n            \"\"\"\n    cursor.execute(sql, (status, post_id))\n    conn.commit()\n    cursor.close()\n    conn.close()\n    print(\"status updated successfully\")\n\n\n#approve posts\ndef approve_post(post_id):\n    conn = get_connection()\n    cursor = conn.cursor()\n    sql = \"UPDATE posts SET status = 'scheduled' WHERE id = %s\"\n    cursor.execute(sql, (post_id,))\n    conn.commit()\n    cursor.close()\n    conn.close()\n\n\n\n\n# id, niche_id, content, approved, posted, created_at, account_id, media_url, status, scheduled_time\n# id, platform, api_key, api_secret, access_token, access_secret\n","size_bytes":7160},"backend/__init__.py":{"content":"","size_bytes":0},"main.py":{"content":"from flask import Flask, jsonify, send_from_directory\nfrom backend.scheduler.job_scheduler import assign_posts_to_slot, process_due_posts, run_scheduler\nimport threading\nimport os\n\n# ------------------- Flask App -------------------\napp = Flask(__name__, static_folder='.')\n\n# ------------------- Static Files Route -------------------\n@app.route(\"/\")\ndef index():\n    \"\"\"Serve the main index.html page.\"\"\"\n    return send_from_directory('.', 'index.html')\n\n@app.route(\"/<path:path>\")\ndef serve_static(path):\n    \"\"\"Serve static files (CSS, JS, etc.).\"\"\"\n    if os.path.exists(path):\n        return send_from_directory('.', path)\n    return \"File not found\", 404\n\n# ------------------- API Routes -------------------\n@app.route(\"/run-assign-posts\")\ndef run_assign_posts():\n    \"\"\"Manually trigger assigning AI posts to time slots.\"\"\"\n    try:\n        assign_posts_to_slot()\n        return jsonify({\"status\": \"success\", \"message\": \"Posts assigned successfully\"})\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)})\n\n\n@app.route(\"/run-due-posts\")\ndef run_due_posts():\n    \"\"\"Manually trigger processing due posts.\"\"\"\n    try:\n        process_due_posts()\n        return jsonify({\"status\": \"success\", \"message\": \"Due posts processed successfully\"})\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)})\n\n\n@app.route(\"/start-scheduler\")\ndef start_scheduler():\n    \"\"\"Start the scheduler in a background thread.\"\"\"\n    try:\n        # Run the scheduler in a separate thread so Flask stays responsive\n        threading.Thread(target=run_scheduler, daemon=True).start()\n        return jsonify({\"status\": \"success\", \"message\": \"Scheduler started in background\"})\n    except Exception as e:\n        return jsonify({\"status\": \"error\", \"message\": str(e)})\n\n\n# ------------------- Flask Entry Point -------------------\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n","size_bytes":1957},"config.py":{"content":"import os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nOPENAI_URL = os.getenv(\"OPENAI_URL\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nX_API_KEY = os.getenv(\"X_API_KEY\")\nX_API_SECRET = os.getenv(\"X_API_SECRET\")\nX_ACCESS_TOKEN = os.getenv(\"X_ACCESS_TOKEN\")\nX_ACCESS_SECRET = os.getenv(\"X_ACCESS_SECRET\")\n\n","size_bytes":308},"db_models.py":{"content":"\nfrom backend.scheduler.job_scheduler import assign_posts_to_slot\nfrom backend.database.models import get_draft_posts_for_niche, get_due_posts\nfrom backend.scheduler.job_scheduler import process_due_posts\nfrom backend.services.ai_generator import generate_post_content\n\n#create_niche(\"tech\", \"all about tech\", \"strictly about tech\")\n\n# get_all_niches()\n\n#create_hashtags(1, \"remove tinubu\")\n\n#niches = get_all_hashtags(\"1\")\n#print(niches)\n\n#create_account(\"x\", \"aljf98woeijsfkjew9834weojaf\", \"lasjflkjsldkffa\", \"lsfkjslafksfj\", \"sakjflasfie\")\n\n#accounts = get_all_accounts()\n#print(accounts)\n\n\n\n#post = get_all_accounts()\n#print(post)\n\n#post = get_scheduled_posts()\n#print(post)\n\n#create_schedule(\"1\", \"3\", \"1\")\n\n#niche = get_all_schedule_for_niche(1)\n#print(niche)\n\n#schedule = get_all_schedule()\n#print(schedule)\n#create_post(niche_id = 1, account_id = 1, content = \"i want to be extremely rich\", media_url=None, scheduled_time = \"2025-09-21\", status=\"draft\" )\n\n\n#posts = get_all_posts()\n#print(posts)\n\n#count = count_scheduled_posts_for_niche_today(1)\n#print(count)\n\n#print(generate_time_slots(5))\n#print(generate_time_slots(4))\n#print(generate_time_slots(4))\n\n#print(get_draft_posts_for_niche(1, 5))\n\n\nprint(assign_posts_to_slot())\n\n#status = get_due_posts()\n#print(status)\n\n\n\n#status = process_due_posts()\n#print(status)\n#niche_id, account_id, content, media_url=None, scheduled_time=None, status=\"draft\"\n\n#generate_post_content(\"technology\")\n\n","size_bytes":1449},"db.py":{"content":"import psycopg2\nimport psycopg2.extras\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()  # loads .env file\n\ndef get_connection():\n    # Use DATABASE_URL if available, otherwise construct from individual env vars\n    database_url = os.getenv(\"DATABASE_URL\")\n    \n    if database_url:\n        return psycopg2.connect(database_url)\n    else:\n        return psycopg2.connect(\n            host=os.getenv(\"PGHOST\", \"localhost\"),\n            user=os.getenv(\"PGUSER\", \"postgres\"),\n            password=os.getenv(\"PGPASSWORD\", \"password\"),\n            database=os.getenv(\"PGDATABASE\", \"postgres\")\n        )\n","size_bytes":605},"job_scheduler.py":{"content":"import tweepy\nimport time\nimport schedule\nimport logging\nfrom datetime import datetime\nfrom config import X_API_KEY, X_API_SECRET, X_ACCESS_TOKEN, X_ACCESS_SECRET\nfrom backend.database.models import (\n    get_all_schedule_for_niche,\n    get_draft_posts_for_niche,\n    update_post_schedule,\n    get_due_posts,\n    update_post_status,\n)\nfrom backend.services.ai_generator import generate_post_content\n\n# ------------------- Logging Setup -------------------\nlogging.basicConfig(\n    filename=\"scheduler.log\",\n    level=logging.INFO,\n    format=\"%(asctime)s — %(levelname)s — %(message)s\",\n)\n\n\n# ------------------- Time Slot Generator -------------------\ndef generate_time_slots(posts_per_day):\n    today = datetime.today().date()\n    start = datetime.combine(today, datetime.strptime(\"07:00\", \"%H:%M\").time())\n    end = datetime.combine(today, datetime.strptime(\"23:00\", \"%H:%M\").time())\n\n    if posts_per_day <= 0:\n        return []\n\n    duration = end - start\n    interval = duration / posts_per_day\n    slots = [(start + (interval * i)).strftime(\"%Y-%m-%d %H:%M:%S\") for i in range(posts_per_day)]\n    return slots\n\n\n# ------------------- Assign Posts -------------------\ndef assign_posts_to_slot():\n    schedule_data = get_all_schedule_for_niche(1)\n    logging.info(f\"schedule for niche: {schedule_data}\")\n\n    if not schedule_data:\n        logging.warning(\"No schedule found.\")\n        return\n\n    schedule_row = schedule_data[0]\n    posts_per_day = schedule_row[\"posts_per_day\"]\n    needs_approval = schedule_row[\"needs_approval\"]\n    slots = generate_time_slots(posts_per_day)\n    drafts = get_draft_posts_for_niche(1, posts_per_day)\n\n    if not drafts:\n        logging.warning(\"No draft posts found for scheduling.\")\n        return\n\n    for slot, draft in zip(slots, drafts):\n        niche_id = draft[\"niche_id\"]\n        ai_text = generate_post_content(niche_id)\n\n        try:\n            if needs_approval == 1:\n                update_post_schedule(draft[\"id\"], slot, status=\"draft\", content=ai_text)\n                logging.info(f\"Draft post {draft['id']} scheduled with AI content.\")\n            else:\n                update_post_schedule(draft[\"id\"], slot, status=\"scheduled\", content=ai_text)\n                logging.info(f\"Scheduled post {draft['id']} updated with AI content.\")\n        except Exception as e:\n            logging.error(f\"Error updating post {draft['id']}: {e}\")\n\n\n# ------------------- Process Due Posts -------------------\ndef process_due_posts():\n    due_posts = get_due_posts()\n    if not due_posts:\n        logging.info(\"No due posts found.\")\n        return\n\n    for due in due_posts:\n        needs_approval = due[\"needs_approval\"]\n        post_id = due[\"id\"]\n\n        try:\n            if needs_approval == 1 and due[\"status\"] == \"scheduled\":\n                update_post_status(post_id, \"pending approval\")\n                logging.info(f\"Post {post_id} set to pending approval.\")\n            elif due[\"status\"] == \"approved\":\n                publish_post(due)\n            else:\n                publish_post(due)\n                update_post_status(post_id, \"posted\")\n        except Exception as e:\n            logging.error(f\"Error processing post {post_id}: {e}\")\n\n\n# ------------------- Publish Post -------------------\ndef publish_post(post):\n    post_id = post[\"id\"]\n    content = post[\"content\"]\n\n    client = tweepy.Client(\n        consumer_key=X_API_KEY,\n        consumer_secret=X_API_SECRET,\n        access_token=X_ACCESS_TOKEN,\n        access_token_secret=X_ACCESS_SECRET,\n    )\n\n    try:\n        me = client.get_me()\n        logging.info(f\"Authenticated as: {me.data.username}\")\n\n        response = client.create_tweet(text=content)\n        logging.info(f\"Tweet successfully posted for post {post_id}: {response}\")\n        update_post_status(post_id, \"posted\")\n\n    except tweepy.TooManyRequests:\n        logging.warning(\"Rate limit reached. Retrying in 15 minutes...\")\n        time.sleep(900)\n        publish_post(post)\n    except Exception as e:\n        logging.error(f\"Error posting tweet {post_id}: {e}\")\n\n\n# ------------------- Run Scheduler -------------------\ndef run_scheduler():\n    schedule.every().day.at(\"00:00\").do(assign_posts_to_slot)\n    schedule.every(1).minutes.do(process_due_posts)\n\n    logging.info(\"Scheduler started... press CTRL+C to stop.\")\n    while True:\n        schedule.run_pending()\n        time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    run_scheduler()\n","size_bytes":4431},"replit.md":{"content":"# Auto Poster - Social Media Automation Platform\n\n## Overview\nThis is a social media auto-poster application that uses AI to generate and schedule posts across social media platforms. The application features a Flask backend with a PostgreSQL database and supports automated posting with AI-generated content.\n\n## Tech Stack\n- **Backend**: Flask (Python 3.11)\n- **Database**: PostgreSQL (via Replit's managed database)\n- **AI**: OpenAI API for content generation\n- **Social Media**: Twitter/X API via Tweepy\n- **Scheduling**: APScheduler & schedule libraries\n- **Frontend**: HTML/CSS static files\n\n## Project Structure\n```\n.\n├── backend/\n│   ├── database/\n│   │   ├── models.py        # Database models and queries\n│   │   └── __init__.py\n│   ├── routes/\n│   │   └── register.py\n│   ├── scheduler/\n│   │   ├── job_scheduler.py # Scheduling logic\n│   │   └── __init__.py\n│   ├── services/\n│   │   ├── ai_generator.py  # AI content generation\n│   │   ├── post_generator.py\n│   │   └── x_api_service.py # Twitter/X API integration\n│   └── utils/\n│       └── logger.py\n├── main.py                   # Flask application entry point\n├── db.py                     # Database connection\n├── config.py                 # Configuration management\n├── schema.sql               # Database schema\n├── index.html               # Frontend homepage\n├── style.css                # Styles\n└── requirements.txt         # Python dependencies\n\n## Recent Changes (October 17, 2025)\n1. **Database Migration**: Converted from MySQL to PostgreSQL\n   - Updated `db.py` to use `psycopg2` instead of `mysql-connector-python`\n   - Updated all database models to use PostgreSQL-compatible syntax\n   - Replaced MySQL-specific functions (`NOW()` → `CURRENT_TIMESTAMP`, `CURDATE()` → `CURRENT_DATE`)\n   - Changed cursor factory from `dictionary=True` to `psycopg2.extras.RealDictCursor`\n\n2. **Flask Server Enhancement**: \n   - Added static file serving routes\n   - Configured to serve `index.html` at root route\n   - Added route to serve CSS and other static assets\n\n3. **Replit Environment Setup**:\n   - Installed Python 3.11 and all required dependencies\n   - Created PostgreSQL database schema with all necessary tables\n   - Configured Flask workflow to run on port 5000\n   - Set up deployment configuration for VM (always-on server)\n\n4. **Environment Configuration**:\n   - Created `.env.example` file with required API keys\n   - Updated `.gitignore` for Python project best practices\n\n## Database Schema\nThe application uses the following tables:\n- **niches**: Content categories/topics\n- **hashtags**: Associated hashtags for niches\n- **platform_accounts**: Social media account credentials\n- **posts**: Generated posts with scheduling info\n- **schedule**: Posting schedule configuration per niche\n\n## API Endpoints\n- `GET /` - Homepage\n- `GET /run-assign-posts` - Manually trigger post assignment to time slots\n- `GET /run-due-posts` - Manually process due posts\n- `GET /start-scheduler` - Start the background scheduler\n\n## Required Environment Variables\n```\nOPENAI_URL=https://api.openai.com/v1\nOPENAI_API_KEY=your_openai_api_key_here\nX_API_KEY=your_x_api_key_here\nX_API_SECRET=your_x_api_secret_here\nX_ACCESS_TOKEN=your_x_access_token_here\nX_ACCESS_SECRET=your_x_access_secret_here\n```\n\nDatabase variables are automatically provided by Replit:\n- `DATABASE_URL`\n- `PGHOST`\n- `PGDATABASE`\n- `PGUSER`\n- `PGPASSWORD`\n\n## How to Run\nThe Flask server runs automatically via the configured workflow:\n```bash\npython main.py\n```\n\nThe server will start on `http://0.0.0.0:5000`\n\n## Deployment\n- **Deployment Type**: VM (always-on)\n- **Command**: `python main.py`\n- The application is configured for production deployment with Replit's managed infrastructure\n\n## Features\n1. **AI Content Generation**: Uses OpenAI to generate social media posts\n2. **Automated Scheduling**: Posts are scheduled based on configurable time slots\n3. **Multi-Platform Support**: Designed to support multiple social media platforms\n4. **Approval Workflow**: Optional approval required before posting\n5. **Background Scheduler**: Runs continuously to process and post content\n\n## Notes\n- The application was migrated from MySQL to PostgreSQL for Replit compatibility\n- The scheduler runs in a background thread to keep Flask responsive\n- Posts can be scheduled with or without manual approval based on niche settings\n","size_bytes":4538}},"version":2}